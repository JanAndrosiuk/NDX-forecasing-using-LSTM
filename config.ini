[logger]
LoggerLevel = 20
TensorboardLoggerPath = reports/tblogs

[raw]
IcsaRawDF = data/raw/icsa.csv
CandleRawDF = data/raw/ndx.csv

[prep]
DataPreprocessedDir = data/preprocessed/
DataInputDir = data/input/
DataOutputDir = data/output/
ReportDir = reports/
VisualizationsDir = reports/figures/
FFilledIcsaDfPkl = data/preprocessed/icsa_ffilled.pkl
FFilledIcsaDfCsv = data/preprocessed/icsa_ffilled.csv
TisDfPkl = data/preprocessed/tis.pkl
TisDfCsv = data/preprocessed/tis.csv
JoinedDfPkl = data/input/joined.pkl
JoinedDfCsv = data/input/joined.csv
WindowSplitDict = data/input/window_split.pkl
PredictionsArray = data/output/latest_preds.pkl

[model]
;Seed = 1337
;
VarTarget = Target
; Features = Close
; Features = Open, High, Low, Close, Volume
Features = ICSA, SMA5, SMA10, SMA50, EMA20, stoch5, ADOSC, MACDhist, WILLR, RSI, MOM, ROC, OBV, CCI, Open, High, Low, Close, Volume
; 
HyperParamTuneTrials = 30
; 
Lookback = 63
TrainWindow = 504
ValidationWindow = 63
TestWindow = 126
; 
BatchSizeTrain = 63
BatchSizeValidation = 63
BatchSizeTest = 63
PredictionThreshold = 0
; 
Epochs = 50
; 
LSTMUnitsMin = 32
LSTMUnitsMax = 512
; 
HiddenLayersMin = 1
HiddenLayersMax = 3
; 
DropoutRateMin = 0.05
DropoutRateMax = 0.5
; 
Problem = regression
LossFunctionClassification = Hinge
LossFunctionRegression = MAPE, MSE
LossMinDeltaMSE = 2.5e-4
LossMinDeltaMape = 50
; 
ActivationFunction = tanh
; 
LearningRate = 0.05, 0.025, 0.01, 0.005, 0.001
LearningRateDecay = 0.01
Optimizer = Adam, RMSprop, Adadelta
AdamWeightDecay = 0.1
TargetThreshold = 0.001

[evaluation]
TransactionCost = 0.0005
